x-superset-build: &superset-build
  context: ./superset/
  dockerfile: Dockerfile
  target: dev
x-superset-depends-on: &superset-depends-on
  - db
  - redis
x-superset-volumes: &superset-volumes
  - ./superset/docker/docker-init.sh:/app/docker-init.sh
  - ./superset/docker/pythonpath_dev:/app/pythonpath

version: "3.7"
services:
  master:
    build: '.'
#    image: panovvv/bigdata-base-image:2.1
    hostname: master
    depends_on:
      - hivemetastore
    environment:
      HADOOP_NODE: namenode
      HIVE_CONFIGURE: yes, please
      SPARK_PUBLIC_DNS: localhost
      SPARK_LOCAL_IP: 172.28.1.1
      SPARK_MASTER_HOST: 172.28.1.1
      SPARK_LOCAL_HOSTNAME: master
    expose:
      - 1-65535
    ports:
      # Spark Master Web UI
      - 127.0.0.1:8080:8080
      # Spark job Web UI: increments for each successive job
      - 127.0.0.1:4040:4040
      - 127.0.0.1:4041:4041
      - 127.0.0.1:4042:4042
      - 127.0.0.1:4043:4043
      # YARN UI
      - 127.0.0.1:8088:8088
      # Hadoop namenode UI
      - 127.0.0.1:9870:9870
      # Hadoop secondary namenode UI
      - 127.0.0.1:9868:9868
      # Hive JDBC
      - 127.0.0.1:10000:10000
    volumes:
      - ./data:/data
    networks:
      spark_net:
        ipv4_address: 172.28.1.1
    extra_hosts:
      - "worker1:172.28.1.2"
      - "worker2:172.28.1.3"
      - "hivemetastore:172.28.1.4"
      - "zeppelin:172.28.1.5"
      - "livy:172.28.1.6"

  worker1:
    build: '.'
#    image: panovvv/bigdata-base-image:2.1
    hostname: worker1
    depends_on:
      - hivemetastore
    environment:
      SPARK_MASTER_ADDRESS: spark://master:7077
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
      SPARK_LOCAL_HOSTNAME: worker1
      SPARK_LOCAL_IP: 172.28.1.2
      SPARK_MASTER_HOST: 172.28.1.1
      HADOOP_NODE: datanode
    expose:
      - 1-65535
    ports:
      # Hadoop datanode UI
      - 127.0.0.1:9864:9864
      #Spark worker UI
      - 127.0.0.1:8081:8081
    volumes:
      - ./data:/data
    networks:
      spark_net:
        ipv4_address: 172.28.1.2
    extra_hosts:
      - "master:172.28.1.1"
      - "worker2:172.28.1.3"
      - "hivemetastore:172.28.1.4"
      - "zeppelin:172.28.1.5"
      - "livy:172.28.1.6"

  worker2:
    build: '.'
#    image: panovvv/bigdata-base-image:2.1
    hostname: worker2
    depends_on:
      - hivemetastore
    environment:
      SPARK_MASTER_ADDRESS: spark://master:7077
      SPARK_WORKER_PORT: 8882
      SPARK_WORKER_WEBUI_PORT: 8082
      SPARK_PUBLIC_DNS: localhost
      SPARK_LOCAL_HOSTNAME: worker2
      SPARK_LOCAL_IP: 172.28.1.3
      SPARK_MASTER_HOST: 172.28.1.1
      HADOOP_NODE: datanode
      HADOOP_DATANODE_UI_PORT: 9865
    expose:
      - 1-65535
    ports:
      # Hadoop datanode UI
      - 127.0.0.1:9865:9865
      # Spark worker UI
      - 127.0.0.1:8082:8082
    volumes:
      - ./data:/data
    networks:
      spark_net:
        ipv4_address: 172.28.1.3
    extra_hosts:
      - "master:172.28.1.1"
      - "worker1:172.28.1.2"
      - "hivemetastore:172.28.1.4"
      - "zeppelin:172.28.1.5"
      - "livy:172.28.1.6"

  hivemetastore:
    image: postgres:11.5
    hostname: hivemetastore
    environment:
      POSTGRES_PASSWORD: new_password
    expose:
      - 5432
    volumes:
      - ./conf/hive/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      spark_net:
        ipv4_address: 172.28.1.4
    extra_hosts:
      - "master:172.28.1.1"
      - "worker1:172.28.1.2"
      - "worker2:172.28.1.3"
      - "zeppelin:172.28.1.5"
      - "livy:172.28.1.6"

  zeppelin:
    build: './docker_zeppelin'
#    image: panovvv/bigdata-zeppelin:2.1
    hostname: zeppelin
    depends_on:
      - master
      - worker1
      - worker2
    volumes:
      - ./zeppelin_notebooks:/zeppelin_notebooks
      - ./data:/data
    environment:
      ZEPPELIN_PORT: 8890
      ZEPPELIN_NOTEBOOK_DIR: '/zeppelin_notebooks'
      HADOOP_NODE: none
    expose:
      - 8890
    ports:
      - 127.0.0.1:8890:8890
    networks:
      spark_net:
        ipv4_address: 172.28.1.5
    extra_hosts:
      - "master:172.28.1.1"
      - "worker1:172.28.1.2"
      - "worker2:172.28.1.3"
      - "hivemetastore:172.28.1.4"
      - "livy:172.28.1.6"

  livy:
    build: './docker_livy'
#    image: panovvv/bigdata-livy:2.1
    hostname: livy
    depends_on:
      - master
      - worker1
      - worker2
    volumes:
      - ./livy_batches:/livy_batches
      - ./data:/data
    environment:
      HADOOP_NODE: none
    expose:
      - 1-65535
    ports:
      - 127.0.0.1:8998:8998
    networks:
      spark_net:
        ipv4_address: 172.28.1.6
    extra_hosts:
      - "master:172.28.1.1"
      - "worker1:172.28.1.2"
      - "worker2:172.28.1.3"
      - "hivemetastore:172.28.1.4"
      - "zeppelin:172.28.1.5"
  flume:
    tty: true
    build: './flume'
    hostname: flume
    depends_on:
      - master
    volumes:
      - ./data:/data
    environment:
      HADOOP_NODE: none
    expose:
      - 1-65535
    networks:
      spark_net:
        ipv4_address: 172.28.1.7
    extra_hosts:
      - "master:172.28.1.1"
      - "worker1:172.28.1.2"
      - "worker2:172.28.1.3"
      - "hivemetastore:172.28.1.4"
      - "zeppelin:172.28.1.5"
      - "livy:172.28.1.6"

  kettle:
    build: './kettle'
    hostname: kettle
    volumes:
      - kettle:/home/tomcat/.kettle
      - pentaho:/home/tomcat/.pentaho
    environment:
      - "JAVA_OPTS=-Xms1024m -Xmx2048m"
    expose:
      - 1-65535
    ports:
      - 127.0.0.1:7070:8080
    networks:
      spark_net:
        ipv4_address: 172.28.1.8
    extra_hosts:
      - "master:172.28.1.1"
      - "worker1:172.28.1.2"
      - "worker2:172.28.1.3"
      - "hivemetastore:172.28.1.4"
      - "zeppelin:172.28.1.5"
      - "livy:172.28.1.6"
      - "flume:172.28.1.7"

  redis:
    image: redis:3.2
    hostname: redis
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"
    networks:
      spark_net:
        ipv4_address: 172.28.1.10
    volumes:
      - redis:/data

  db:
    env_file: superset/docker/.env
    image: postgres:10
    hostname: db
    restart: unless-stopped
    ports:
      - "127.0.0.1:5432:5432"
    networks:
      spark_net:
        ipv4_address: 172.28.1.11
    volumes:
      - db_home:/var/lib/postgresql/data

  superset:
    hostname: superset
    build: *superset-build
    command: ["flask", "run", "-p", "8088", "--with-threads", "--reload", "--debugger", "--host=0.0.0.0"]
    env_file: superset/docker/.env
    restart: unless-stopped
    ports:
      - 127.0.0.1:7088:8088
    networks:
      spark_net:
        ipv4_address: 172.28.1.12
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    extra_hosts:
      - "master:172.28.1.1"
      - "redis:172.28.1.10"
      - "db:172.28.1.11"

  superset-init:
    hostname: superset-init
    build: *superset-build
    command: ["/app/docker-init.sh"]
    env_file: superset/docker/.env
    networks:
      spark_net:
        ipv4_address: 172.28.1.13
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    extra_hosts:
      - "redis:172.28.1.10"
      - "db:172.28.1.11"

  superset-node:
    hostname: superset-node
    image: node:10-jessie
    command: ["bash", "-c", "cd /app/superset/assets && npm install && npm run dev"]
    env_file: superset/docker/.env
    networks:
      spark_net:
        ipv4_address: 172.28.1.14
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    extra_hosts:
      - "redis:172.28.1.10"
      - "db:172.28.1.11"
 
  superset-worker:
    hostname: superset-worker
    build: *superset-build
    command: ["celery", "worker", "--app=superset.tasks.celery_app:app", "-Ofair"]
    env_file: superset/docker/.env
    restart: unless-stopped
    networks:
      spark_net:
        ipv4_address: 172.28.1.15
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    extra_hosts:
      - "redis:172.28.1.10"
      - "db:172.28.1.11"

networks:
  spark_net:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
volumes:
  kettle:
  pentaho:
  superset_home:
    external: false
  db_home:
    external: false
  redis:
    external: false
